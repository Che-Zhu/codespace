# Machine Learning Workflow

Machine Learning includes many types of task and possible algorithms, but all rely on a key assumption:
* Existing data will apply (generalise) to new data.
* In other words, we can predict outcomes for new data, based on existing data
    * Goal is to measure **generalisation error**

## Problems with Data

### Quantity of Data
It is hard to determine how much data is enough, it depends on the complexity of the problem, as well as the dimension of the data.

For example, if we were to predict the life satisfaction of a country based on it's GDP, we don't need much data, and the minimun data points required is 2 (in theory).

But if we are classifying images, we may need millions of data (images) to make a good model.

### Nonrepresentative Training Data
In order to generalize well, it is crucial that the training data be representative of the new cases we want to generalize to.

**Example**
![Nonrepresentative Training data](https://cdn.jsdelivr.net/gh/Che-Zhu/graveyard-of-pics@main/img/20210305003821.png)

The dotted line represents the linear model trained without the three countries on the right. As we can see, this model is unlikely to make accurate predictions as the training set itself is nonrepresentative. In fact, such a simple linear model is probably never going to work well,

### Poor Quality Data
Real data may contains errors, outliers, and noise, and it is often well worth the effort to spend time cleaning up the training data. The following are a couple of examples of how to clean up training data:
* If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually.
* If some instances are missing a few features (e.g. 5% of your customers did not specify their age), you must decide whether you want to ignore this attribute altogether, ignore these instances, fill in the missing values (e.g. with the median age), or train one model with the feature and one model without it.

### Irrelevant Features
A crritical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called **feature engineering**, involves the following steps:
* Feature selection (selecting the most useful features to train on among existing features)
* Feature extraction (combining existing features to preduce a more useful one)
* Creating new featurews by gathering new data

## Problems with models

### Overfitting
* Model is too complex for the data
* Essentially fitting to noise in the data

![](https://cdn.jsdelivr.net/gh/Che-Zhu/graveyard-of-pics@main/img/20210305170144.png)

This problem can be solved by:
* Collect more data
* Reduce noise
* Choose a less complex model with fewer parameters
* Regularisation
    * "Soft" reduction in parameters
    * Penalise paramter values that are far from zero

#### Regularisation
![](https://cdn.jsdelivr.net/gh/Che-Zhu/graveyard-of-pics@main/img/20210305164928.png)

In this example, we can see that the regularization forced the model to have a smaller slope: this model does not fit the training data (circles) as well as the first model (Linear model on partial data), but it actually generalizes better to new examples (squares) that it did not see during trainings.

### Underfitting
The model is too simple to capture patterns in the data.

![](https://cdn.jsdelivr.net/gh/Che-Zhu/graveyard-of-pics@main/img/20210305165547.png)

As we can see from the picture above, the model (linear line) is too simple to learn the underlying structure of the data. The reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.

Main options to fix this problem:
* Select a more powerful model, with more parameters.
* Feed better features to the learning algorithm (feature engineering)
* Reduce the constraints on the model (e.g. reduce the regularization hyperparameter).

## Selecting a model
We want a model that generalises from exisiting data to new, unseen cases. but it is impossible to know in advance what model to choose in most cases. So we pick a few likely candidates and select the best model by comparing them.
* This requires us to measure generalisation error

### Training and test sets
Assuming the data we have is representative of future data, we then split it:
* Training set, used to fit model parameters
* Test set, used to measure prediction accuracy after fitting (generalisation error)

After the model has been trained on training set, we evaluate the model by measure the prediction error on both training and test sets:
* High training set error - underfitting
* Low training error, high test set error - overfitting

### Model Validation
Problem with testing on test sets:
* By training and testing on the same train/test split, we select for best performance **on this split**, this means that the model is unlikely to perform as well on new data. Hence the test set should not be used during training process.

A common solution to this problem is called holdout validation:

We take one extra part of the training set and name it: Validation set.

Then train multiple models with various hyperparameters on the reduced training set (Full set - test set - validation set), and select the model that performs best on the validation set.

Then we train the best model on the full training set (including validation set), and this gives you the final model. Lastly, we evaluate this final model on the test set to get an estimate of the generalization error.

![](https://cdn.jsdelivr.net/gh/Che-Zhu/graveyard-of-pics@main/img/20210305202438.png)

### Cross Validation (standard)
Cross validation:
* Gives us multiple error measures per model
* Can compute variance, error ranges etc
* More robust with smaller training sets, not dependent on a single split
* But more time consuming, as we have to train the models many times

Finally re-train model with best settings on the full training set, and apply that model to test set, to estimate generalisation error

## MAchine Learning Workflow
1. Check data for common problems
    * e.g. missing values, invalid values, outliers, noise, scale, etc.
2. Choose some candidate models based on data and task
3. Split data into training and test sets
4. Split training data into (reduced) training and validation sets
    * Possibly multiple ways, for cross validation
5. Train candidate models on training sets
6. Select best model based on validation set errors
7. Retrain model on the full training set
8. Apply best model to test data
    * This gives estimate of the generalisation error, and hopefully that is about the same generalisation error when we apply this model to unseen data.

## Dealing with data
### Missing data
sklearn supports other strategies by **inputers**

for example, replace missing features with median feature value:

``` python
from sklearn.inpute import SimpleInputer

imputer = SimpleInputer(strategy='median')

# Calculate the median for each feature
inputer.fit(training_features)

# Fill missing data (NaN) with median value
filled_features = inputer.transform(training_features)
```

### Scaling data
Scale all features to range [0,1]

``` python
from sklearn.preprocessing import MinMaxScaler

# Default range is [0,1]
scaler = MinMaxScaler()

# Find min and max value for each feature
scaler.fit(training_features)

# Apply scaling to each feature
scaled_features = scaler.transform(training_features)
```

#### Standardised scaling
scale each feature to have mean = 0, variance 1

``` python
from sklearn.preprocessing import StandardScaler

# Default is mean 0, variance 1
scaler = StandardScaler()

# Find mean and variance for each feature
scaler.fit(training_features)

# Apply scaling to each feature
scaled_features = scaler.transform(training_features)
```

## Pipelines in sklearn
We can use sklearn pipeline to apply the same sequence of steps to multiple data sets. (Convenient to apply the same processing steps to the test data)

``` python
from sklearn.pipeline import Pipeline

# Replace missing features with median, and scale to std distribution
std_pipeline = Pipeline([ ('inputer', SimpleImputer(strategy='median'))
                          ('std_scaler', StandardScaler())])

transformed_features = std_pipeline.fit_transform(training_features)

...

# Apply the same transformations to test data
trans_test_features = std_pipeline.fit_transform(test_features)
```