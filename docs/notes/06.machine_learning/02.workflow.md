# Machine Learning Workflow

Machine Learning includes many types of task and possible algorithms, but all rely on a key assumption:
* Existing data will apply (generalise) to new data.
* In other words, we can predict outcomes for new data, based on existing data
    * Goal is to measure **generalisation error**

## Problems with Data

### Quantity of Data
It is hard to determine how much data is enough, it depends on the complexity of the problem, as well as the dimension of the data.

For example, if we were to predict the life satisfaction of a country based on it's GDP, we don't need much data, and the minimun data points required is 2 (in theory).

But if we are classifying images, we may need millions of data (images) to make a good model.

### Nonrepresentative Training Data
In order to generalize well, it is crucial that the training data be representative of the new cases we want to generalize to.

**Example**
![Nonrepresentative Training data](https://cdn.jsdelivr.net/gh/Che-Zhu/graveyard-of-pics@main/img/20210305003821.png)

The dotted line represents the linear model trained without the three countries on the right. As we can see, this model is unlikely to make accurate predictions as the training set itself is nonrepresentative. In fact, such a simple linear model is probably never going to work well,

### Poor Quality Data
Real data may contains errors, outliers, and noise, and it is often well worth the effort to spend time cleaning up the training data. The following are a couple of examples of how to clean up training data:
* If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually.
* If some instances are missing a few features (e.g. 5% of your customers did not specify their age), you must decide whether you want to ignore this attribute altogether, ignore these instances, fill in the missing values (e.g. with the median age), or train one model with the feature and one model without it.

### Irrelevant Features
